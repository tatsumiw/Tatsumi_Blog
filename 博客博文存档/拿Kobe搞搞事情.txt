今天Tatsumi带大家用科比的数据做一个相对完整的数据挖掘项目的小案例，涉及到数据预处理、数据可视化、常用分类模型的构建及相关调参的操作。

（为什么要拿我科的数据集来玩？因为不想用鸢尾花啊，Tatsumi也很想知道究竟能不能很好地预测科比每一次的投篮是否能命中）

首先是从NBA的统计网站上下载了一份科比职业生涯出手及得分的数据，数据集大概是长这样子：

里面涉及的指标有像是科比投篮的类型、投篮位置的坐标、投篮距离篮筐的距离、每一节剩余的分钟数和秒数、投篮的分值和对阵的队伍、比赛的时间等等。

1.首先的话这里先读入数据，并且对数据做一些简单的预处理：
#读入数据
data = pd.read_csv('Kobe_data.csv')
data.head()

#删除没有用的列
dorp = ['action_type','game_event_id','game_id','team_id','team_name','matchup','shot_id','season']
data = data.drop(dorp,axis = 1)

#缺失值检查
data.isnull().sum()#返回每列包含的缺失值的个数
data = data.dropna()

#对数据进行预处理
data['opponent'].unique()
data.columns.tolist()

#对自变量进行独热编码
data2 = pd.get_dummies(data.iloc[:,0:17])
data2.columns.tolist()
2.然后可以做一些简单的特征工程（特征工程和数据质量是模型效果好坏的基石）

这里简单起见，Tatsumi只利用了原始数据的赛季时间计算出科比的年龄（如果想让模型预测效果更好，构造更多有效的特征是非常重要的，大家在实际项目中可以多体会）

#计算科比年龄1978
data['age'] = data['game_date'].apply(lambda x: int(x.split('-')[0]))-1978
data = data.drop('game_date',axis = 1)
3.数据指标的可视化

科比投篮距离的分布图

import seaborn as sns
sns.set( palette="muted", color_codes=True) #设置图风格
sns.distplot(data.shot_distance,bins = 20)


看来科比大佬篮下得分的比例还是相当高的啊。

科比生涯其他指标的展示：

sns.barplot(x=data.shot_zone_area,y=data.shot_made_flag)
sns.barplot(x=data.shot_type,y=data.shot_made_flag)
sns.barplot(x=data.shot_made_flag,y= data.opponent)
sns.barplot(x=data.shot_made_flag,y= data.combined_shot_type)




上面展示了科比得分的倾向（位置、分值、投篮的姿势、对阵的队伍）

详细展示科比出手的区域

#科比投篮
plt.figure(figsize=(10,15))
a = data.groupby('shot_zone_basic')
b = ['silver','orange','r','gold','dodgerblue','brown','gold']
for a1, b1 in zip(a,b):
    plt.scatter(a1[1].loc_x,a1[1].loc_y,color = b1 ,alpha=0.1)


多变量的探索（探索科比年龄与命中率、年龄与投篮距离关系）

sns.pointplot(x=data.age,y= data.shot_made_flag,hue = data.shot_type,markers =['^','o'],linestyle =['-','--'] )

sns.jointplot(data.age,data.shot_distance, kind='reg')


从图中可以简单的看出科比的2分球命中率巅峰主要在28到35岁之间，三分球命中率的巅峰在25岁；而年龄的增大与科比选择投篮的位置并没直接的相关关系。

4.训练测试集的划分，模型的构建，及参数的选择

这里tatsumi使用的模型包括Logistics回归、决策树、SVM、随机森林、Adaboost、GDBT和Xgboost，尽量把各种分类模型的实现一遍，比较各模型的效果并且熟悉模型的参数。

#进行训练集与测试集的划分
data3.shape
X , y = data3.iloc[:,0:75].values , data3.iloc[:,-1].values
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size= 0.3,random_state=1)

#使用逻辑回归
from sklearn.linear_model import LogisticRegression
clf = GridSearchCV(estimator=LogisticRegression(), param_grid=[{'C':(0.01,0.1,1,10),'penalty':('l1','l2')}],scoring='roc_auc', n_jobs = -1,cv = 5)
使用Logistics：
最优参数组合：
C:0.1,class_weight:None?,dual:False,fit_intercept:True?,intercept_scaling:1,,max_iter:100?,

multi_class:'ovr'?,n_jobs:1?,penalty:'l1'?,random_state:None?,solver:'liblinear'?,tol:0.0001?,verbose:0?, warm_start:False

Logistics
???????????	?precision	recall	f1-score	support
0	0.61	0.85	0.71	4232
1	0.64	0.33	0.44	3478
avg / total	0.62	0.61	0.59	7710
#使用SVM
from sklearn.svm import SVC
clf = GridSearchCV(estimator=SVC(max_iter=50,probability = True),param_grid=[{'C':[0.001,0.01,0.1,1,10], 'kernel':['rbf','sigmoid']}], scoring='roc_auc', n_jobs=2, cv = 5)
使用svm：
最优参数组合
C:0.10,cache_size:200,class_weight:None,coef0:0.0,decision_function_shape:'ovr',degree:3,gamma:'auto',kernel:'sigmoid',max_iter:50,
?? ?probability:True,?random_state:None,shrinking:True,?tol:0.001,?verbose:False,

svm
???????????	?precision	recall	f1-score	support
0	0.52	0.71	0.6	4232
1	0.37	0.2	0.26	3478
avg / total	0.45	0.48	0.45	7710
#使用决策树算法
from sklearn import tree
clf = GridSearchCV(estimator=tree.DecisionTreeClassifier(),param_grid=[{'criterion':('gini','entropy'), 'splitter':('best','random'), 'max_depth':range(3,8,1),
                               'min_samples_split':range(2,11,1),'min_samples_leaf':range(2,11,1)}],scoring='roc_auc',n_jobs = -1,cv = 5)
最优参数组合
class_weight:None，?criterion:'entropy'，max_depth:6，max_features:None，max_leaf_nodes:None，min_impurity_decrease:0.0，min_impurity_split:None，
min_samples_leaf:9，min_samples_split:3，min_weight_fraction_leaf:0.0，presort:False，random_state:None，splitter:'random'

决策树
???????????	?precision	recall	f1-score	support
0	0.61	0.86	0.71	4232
1	0.66	0.32	0.43	3478
avg / total	0.63	0.62	0.59	7710
#使用随机森林
from sklearn.ensemble import RandomForestClassifier
clf = GridSearchCV(estimator=RandomForestClassifier(),param_grid=[{'n_estimators':range(10,100,10), 'criterion':('gini','entropy'), 'max_depth':range(3,8,1),
                               'min_samples_split':range(2,11,1), 'min_samples_leaf':range(2,11,1)}],scoring='roc_auc',n_jobs = -1, cv = 5)
最优参数组合
bootstrap:True,class_weight:None,?criterion:'gini',max_depth:7,max_features:'auto',max_leaf_nodes:None,min_impurity_decrease:0.0,min_impurity_split:None,
min_samples_leaf:7,min_samples_split:8,min_weight_fraction_leaf:0.0,n_estimators:90,n_jobs:1,oob_score:False,random_state:None,verbose:0,?warm_start:False

RandomForest
???????????	?precision	recall	f1-score	support
0	0.6	0.85	0.71	4232
1	0.64	0.32	0.43	3478
avg / total	0.62	0.61	0.58	7710
#使用Adaboost
from sklearn.ensemble import AdaBoostClassifier
clf = GridSearchCV(estimator=AdaBoostClassifier(),param_grid=[{'learning_rate':np.linspace(0.1,1.2,6),'n_estimators':range(10,100,10),
                               'algorithm':('SAMME','SAMME.R')}], scoring='roc_auc', n_jobs=-1,cv = 5)
最优参数组合
algorithm:'SAMME.R',base_estimator:None,learning_rate:0.32,n_estimators:80,random_state:None
Adaboost
???????????	?precision	recall	f1-score	support
0	0.61	0.85	0.71	4232
1	0.65	0.32	0.43	3478
avg / total	0.63	0.62	0.58	7710
#使用GBDT
from sklearn.ensemble import GradientBoostingClassifier
clf = GridSearchCV(estimator=GradientBoostingClassifier(),param_grid=[{'learning_rate':np.linspace(0.1,1.2,6), 'n_estimators':range(10,100,10),
                               'criterion':('friedman_mse','mse','mae'), 'max_depth':range(3,8,1), 'min_samples_split':range(2,11,1),'min_samples_leaf':range(2,11,1)}],
         scoring='roc_auc',n_jobs=-1,cv = 5)
最优参数组合
learning_rate=0.1,n_estimators=20,max_depth=3,?min_samples_leaf=60,min_samples_split =1000,max_features='sqrt',? subsample=0.8

GBDT
???????????	?precision	recall	f1-score	support
0	0.6	0.85	0.71	4232
1	0.64	0.32	0.43	3478
avg / total	0.62	0.61	0.58	7710
#使用XGBoost
from xgboost import XGBClassifier
clf = GridSearchCV(estimator=XGBClassifier(),param_grid=[{'max_depth':range(3,9,1),'gamma':[0.1,0.2], 'subsample':np.linspace(0.5,1,5),
                               'colsample_bytree':np.linspace(0.5,1,5),'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100], 'learning_rate':[0.01,0.1,1]}],scoring='roc_auc',
         n_jobs=2, cv = 5)
最优参数组合
base_score:0.5,booster:'gbtree',colsample_bylevel:1,colsample_bytree:1,gamma:0.1,learning_rate:0.1,max_delta_step:0,max_depth:5,?min_child_weight:1,
missing:None,?n_estimators:100,n_jobs:1,nthread:None,objective:'binary:logistic',?random_state:0,reg_alpha:0,reg_lambda:1,scale_pos_weight:1,seed:None,
silent:True,subsample:1

XGBoost
???????????	?precision	recall	f1-score	support
0	0.61	0.85	0.7	4232
1	0.64	0.32	0.44	3478
avg / total	0.62	0.61	0.58	7710
XGBoost模型变量重要性图



各个模型的最后效果

　	Logistics	svm	决策树	RandomForest	Adaboost	GBDT	XGBoost
准确率：	0.61478599	0.48184176	0.61828794	0.61374838	0.61582361	0.61193256	0.61245136
精确率：	0.64048673	0.36557462	0.65634132	0.64400922	0.64793578	0.64062500	0.63506064
召回率：	0.33294997	0.20212766	0.32288672	0.32144911	0.32489937	0.31828637	0.33122484
F1：	0.43813848	0.26032216	0.43283870	0.42884542	0.43278437	0.42527852	0.43537415
AUC:	0.58967914	0.45692394	0.59197266	0.58770943	0.58990715	0.58577362	0.58739881
ROC曲线



通过Kobe生涯的投篮数据简单的过了一遍数据挖掘的大致流程，项目中还有很多可以深究的地方，例如数据集的变量选择，特征工程构造指标，可视化中也还能画出很多其他好看的图，这里Tatsumi都稍微有点略过了，有兴趣的话大家都可以尝试下。

这个项目的理论基础都是大学时期学到的知识，毕业半年终于有时间和经历稍微整理了一下，Kobe退役Tatsumi的大学生活也结束了，用这个小案例算为自己的大学生活o个答复吧。

项目心得：

Tatsumi运用到了常用的7种分类模型，效果上也是略微的有点不同，所以在实际项目中要根据不同的场景选择不同的算法，使用算法前要清楚各种算法的优缺点及适用范围，模型的参数含义也有通过API文档了解清楚。在使用GridSearch时Tatsumi发现还是很考验机子的性能的，所以调参时要按照合理的顺序进行，步长从大到小（Tatsumi案例中的调参方法其实是不可取的，只是简单给大家介绍）

小项目的相关数据和脚本会上传到个人Github

欢迎大家沟通交流学习



